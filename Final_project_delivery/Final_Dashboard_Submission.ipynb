{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Exists: Fetch the latest data from repository\n"
     ]
    }
   ],
   "source": [
    "# %load ./src/import_data.py\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def import_json():\n",
    "    '''Function defintion to import a json dictionary obtained from owid website \n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    dict: json_object_cases \n",
    "        a .json dictionary for each country covid data \n",
    "    '''    \n",
    "    data_cases = requests.get(\n",
    "        'https://covid.ourworldindata.org/data/owid-covid-data.json')\n",
    "    json_object_cases = json.loads(data_cases.content)\n",
    "\n",
    "    return json_object_cases\n",
    "\n",
    "\n",
    "def import_cases_data():\n",
    "    ''' Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predifined csv structure. If there is no Repository \n",
    "        not present then clone the data from GitHub.\n",
    "\n",
    "    Returns:\n",
    "        list: list of all countries by iso code\n",
    "        pandas dataframe: Dataframe for all the country information \n",
    "    '''\n",
    "    #Checking if the path exists or not \n",
    "    if os.path.exists('../data/raw/COVID-19/'):\n",
    "        print('Repository Exists: Fetch the latest data from repository')\n",
    "        git_pull = subprocess.Popen(\"git pull\",\n",
    "                                    cwd=os.path.dirname(\n",
    "                                        '../data/raw/COVID-19/'),\n",
    "                                    shell=True,\n",
    "                                    stdout=subprocess.PIPE,\n",
    "                                    stderr=subprocess.PIPE)\n",
    "        (out, error) = git_pull.communicate()\n",
    "    else:\n",
    "        print('Repository not present. Fetch the entire repository')\n",
    "        git_clone = subprocess.Popen(\"git clone https://github.com/CSSEGISandData/COVID-19.git\",\n",
    "                                     cwd=os.path.dirname('../data/raw/'),\n",
    "                                     shell=True,\n",
    "                                     stdout=subprocess.PIPE,\n",
    "                                     stderr=subprocess.PIPE)\n",
    "        (out, error) = git_clone.communicate()\n",
    "\n",
    "    url = \"https://covid.ourworldindata.org/data/owid-covid-data.csv\"\n",
    "    df_country_info = pd.read_csv(url, sep=',')\n",
    "\n",
    "    # load json object for the total number of COVID cases\n",
    "    json_object_cases = import_json()\n",
    "    #Obtaining the country list from json object keys\n",
    "    countries_list = list(json_object_cases.keys())\n",
    "    # removing the list of countries with no data and updating the country list \n",
    "    country_remove = ['OWID_INT', 'OWID_CYN']\n",
    "    list_cases_country = list(set(countries_list) - set(country_remove))\n",
    "\n",
    "    return list_cases_country, df_country_info\n",
    "\n",
    "\n",
    "def import_vacc_data():\n",
    "    '''Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predifined csv structure. If there is no Repository \n",
    "        not present then clone the data from GitHub.\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframe: Dataframe for vaccination information of all countries \n",
    "    '''  \n",
    "\n",
    "    # Requesting the Vacination data from our world in data:\n",
    "    url_vaccination = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv'\n",
    "\n",
    "    # Dumping all data from json into a variable:\n",
    "    df_vaccination_info = pd.read_csv(url_vaccination, sep=',')\n",
    "\n",
    "    return df_vaccination_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import_json()\n",
    "    import_cases_data()\n",
    "    import_vacc_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 245756\n",
      " Latest date is: 2022-07-26 00:00:00\n",
      "Repository Exists: Fetch the latest data from repository\n"
     ]
    }
   ],
   "source": [
    "# %load ./src/store_relational_data.py\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#ignore warnings if any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the current working directory path and append it\n",
    "path = (os.getcwd()+'\\\\src\\\\')\n",
    "sys.path.append(path)\n",
    "import import_data\n",
    "\n",
    "def store_relational_data():\n",
    "    ''' Transformes the COVID data in a relational data set\n",
    "\n",
    "    '''\n",
    "    #Read the .csv file for the total number of covid cases \n",
    "    data_path = '../data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    pd_raw = pd.read_csv(data_path)\n",
    "\n",
    "    #Obtain the per day cases \n",
    "    time_idx = pd_raw.columns[4:]\n",
    "    # Convert the per day cases into a dataframe \n",
    "    df_plot = pd.DataFrame({\n",
    "        'date': time_idx})\n",
    "    df_input_large = pd_raw['Country/Region'].unique()\n",
    "\n",
    "    #Browse over each country  and generate the total number of cases \n",
    "    for each in df_input_large:\n",
    "        df_plot[each] = np.array(\n",
    "            pd_raw[pd_raw['Country/Region'] == each].iloc[:, 4::].sum(axis=0))\n",
    "    # Drop date from the dataframe        \n",
    "    df = df_plot.drop('date', axis=1)\n",
    "\n",
    "    # Merging the data set over COUNTRY for CODE column for worldmap\n",
    "    df_code = pd.read_csv(\n",
    "        'https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')\n",
    "    world_raw = pd.DataFrame(\n",
    "        {\"COUNTRY\": df_input_large, \"Confirm cases\": df.iloc[-1]})\n",
    "    world_con = pd.merge(world_raw, df_code, on=\"COUNTRY\").drop(\n",
    "        'GDP (BILLIONS)', axis=1)\n",
    "    world_con.to_csv('../data/processed/COVID_CRD.csv', sep=';', index=False)\n",
    "\n",
    "    # Continuation of data preparation\n",
    "    pd_data_base = pd_raw.rename(columns={'Country/Region': 'COUNTRY',\n",
    "                                          'Province/State': 'state'})\n",
    "\n",
    "    pd_data_base['state'] = pd_data_base['state'].fillna('no')\n",
    "\n",
    "    pd_data_base = pd_data_base.drop(['Lat', 'Long'], axis=1)\n",
    "\n",
    "    pd_relational_model_1 = pd_data_base.set_index(['state', 'COUNTRY']) \\\n",
    "        .T                              \\\n",
    "        .stack(level=[0, 1])             \\\n",
    "        .reset_index()                  \\\n",
    "        .rename(columns={'level_0': 'date',\n",
    "                         0: 'confirmed'},\n",
    "                )\n",
    "    pd_relational_model = pd.merge(\n",
    "        pd_relational_model_1, df_code, on=\"COUNTRY\").drop('GDP (BILLIONS)', axis=1)\n",
    "    pd_relational_model['date'] = pd_relational_model.date.astype(\n",
    "        'datetime64[ns]')\n",
    "\n",
    "    pd_relational_model.to_csv(\n",
    "        '../data/processed/20200823_COVID_relational_confirmed.csv', sep=';', index=False)\n",
    "\n",
    "    # SIR model data preparation\n",
    "    sir_plot = pd.DataFrame({\n",
    "        'date': time_idx})\n",
    "    # sir_plot.head()\n",
    "    sir_arr = pd_raw['Country/Region'].unique()\n",
    "    sir_list = sir_arr.tolist()\n",
    "    for each in sir_list:\n",
    "        sir_plot[each] = np.array(\n",
    "            pd_raw[pd_raw['Country/Region'] == each].iloc[:, 4::].sum(axis=0))\n",
    "    # sir_plot.head()\n",
    "\n",
    "    # to convert all the dates into datetime\n",
    "    time_idx = [datetime.strptime(each, \"%m/%d/%y\") for each in sir_plot.date]\n",
    "    # to convert datetime function to string\n",
    "    time_str = [each.strftime('%Y-%m-%d') for each in time_idx]\n",
    "    # time_str[0:5]\n",
    "\n",
    "    # Storing the processed data file and sep';' is a seperator [German std]\n",
    "    sir_plot.to_csv('../data/processed/COVID_sir_flat_table.csv',\n",
    "                    sep=';', index=False)\n",
    "\n",
    "    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_model.date)))\n",
    "\n",
    "    # Processing Data for Cases per pop:\n",
    "    list_cases_country, df_country_info = import_data.import_cases_data()\n",
    "    json_object_cases = import_data.import_json()\n",
    "\n",
    "    #Generate an empty dataframe for countries \n",
    "    df_country = pd.DataFrame()\n",
    "    # Browse over the iso codes of each country \n",
    "    for each in list_cases_country:\n",
    "        df_country_info['iso_code'].unique()\n",
    "        df_country_info['iso_code'] == each\n",
    "        #Concatenate each country to the empty dataframe and reset indices \n",
    "        df_country = pd.concat(\n",
    "            [df_country, df_country_info[df_country_info['iso_code'] == each]],\n",
    "            sort=False)\n",
    "        df_country = df_country.reset_index(drop=True)\n",
    "    #Obtain the list of country name from dataframe \n",
    "    location_list = df_country_info['location'].unique()\n",
    "\n",
    "    #Define an empty dictionary to append the dates\n",
    "    dict_country = {}\n",
    "\n",
    "    # Browse over the country names \n",
    "    for each in location_list:\n",
    "        dict_country.update({\n",
    "            each:\n",
    "            len(df_country_info[df_country_info['location'] == each]['date'])\n",
    "        })\n",
    "    # Adjust the dataframe to consider from the least starting \\\n",
    "    # date to the most recent ending date for all countries\n",
    "    country_name_date = max(dict_country, key=lambda x: dict_country[x])\n",
    "    df_list = df_country_info[df_country_info['location'] ==\n",
    "                              country_name_date].copy()\n",
    "    df_list.reset_index(drop=True)\n",
    "    #Convert the dataframe of dates to datetime \n",
    "    df_list['date'] = pd.to_datetime(df_list['date'], format='%Y-%m-%d')\n",
    "    #Drop irrelevant columns from the dataframe \n",
    "    df_list = df_list.drop(df_list.iloc[:, :3], axis=1).drop(df_list.iloc[:, 4:],\n",
    "                                                             axis=1)\n",
    "    # Browse over the iso codes of each country \n",
    "    for each in list_cases_country:\n",
    "        df_country_info['iso_code'].unique()\n",
    "        df_info = df_country_info[df_country_info['iso_code'] == each]\n",
    "\n",
    "        #Drop irrelevant columns from the dataframe and retain Cases per pop column\n",
    "        df_data = df_info.drop(df_info.iloc[:, :3], axis=1).drop(\n",
    "            df_info.iloc[:, 5:],\n",
    "            axis=1).rename(columns={'total_cases': 'Cases_per_pop_' + each})\n",
    "        #Obtain the population for each country from the json dictionary \n",
    "        pop = json_object_cases[each]['population']\n",
    "        #Divide by the population to obtain cases per population \n",
    "        df_data.iloc[:, 1] = df_data.iloc[:, 1].div(pop, axis=0)\n",
    "        #Convert date in the date dataframe to datetime format\n",
    "        df_data['date'] = pd.to_datetime(df_data['date'], format='%Y-%m-%d')\n",
    "        # join date dateframe wit the cases per pop dataframe \n",
    "        df_list = df_list.join(df_data.set_index('date'), on='date')\n",
    "        # Reset the indices for the entire dataframe \n",
    "        df_list = df_list.reset_index(drop=True)\n",
    "    # Save the generated .csv file in the processed data folder \n",
    "    df_list.to_csv('../data/processed/Cases_pop_NoNaN.csv',\n",
    "                   sep=';', index=False)\n",
    "\n",
    "    # Procesing Vaccination Data:\n",
    "    # import vaccinaton data from owid website \n",
    "    df_vaccination_info = import_data.import_vacc_data()\n",
    "    #Create an empty dataframe for vaccination \n",
    "    df_vaccination = pd.DataFrame()\n",
    "    #Browse over the list of country iso codes \n",
    "    for each in list_cases_country:\n",
    "        df_vaccination_info['iso_code'].unique()\n",
    "        df_vaccination_info['iso_code'] == each\n",
    "        # Concatenate the vaccination inforamtion to the empty dataframe\n",
    "        df_vaccination = pd.concat([\n",
    "            df_vaccination,\n",
    "            df_vaccination_info[df_vaccination_info['iso_code'] == each]\n",
    "        ],\n",
    "            sort=False)\n",
    "        # Reset indices for the vaccination dataframe \n",
    "        df_vaccination = df_vaccination.reset_index(drop=True)\n",
    "    #Obtain the country names from the vaccination dataframe \n",
    "    location_vacc_list = df_vaccination_info['location'].unique()\n",
    "\n",
    "    #Create an empty dictionary for the vaccination information \n",
    "    dict_vacc_country = {}\n",
    "    # Browse over the list of countries \n",
    "    for each in location_vacc_list:\n",
    "        # Append the country names to the dictionary\n",
    "        dict_vacc_country.update({\n",
    "            each:\n",
    "            len(df_vaccination_info[df_vaccination_info['location'] == each]\n",
    "                ['date'])\n",
    "        })\n",
    "\n",
    "    # Adjust the dataframe to consider from the least starting \\\n",
    "    # date to the most recent ending date for all countries\n",
    "    country_vacc_name_date = max(dict_vacc_country,\n",
    "                                 key=lambda x: dict_vacc_country[x])\n",
    "    df_vacc_list = df_vaccination_info[df_vaccination_info['location'] ==\n",
    "                                       country_vacc_name_date].copy()\n",
    "    #Reset the indices for the dataframe\n",
    "    df_vacc_list.reset_index(drop=True)\n",
    "    #Convert the dataframe of dates to datetime \n",
    "    df_vacc_list['date'] = pd.to_datetime(\n",
    "        df_vacc_list['date'], format='%Y-%m-%d')\n",
    "\n",
    "     #Drop irrelevant columns from the dataframe    \n",
    "    df_vacc_list = df_vacc_list.drop(df_vacc_list.iloc[:, :2],\n",
    "                                     axis=1).drop(df_vacc_list.iloc[:, 3:], axis=1)\n",
    "    #Browse over the list of country iso codes \n",
    "    for each in list_cases_country:\n",
    "        df_vaccination_info['iso_code'].unique()\n",
    "        df_vacc_info = df_vaccination_info[df_vaccination_info['iso_code'] == each]\n",
    "        #Drop irrelevant columns from the dataframe and retain Vaccination per pop column\n",
    "        df_vacc_data = df_vacc_info.drop(df_vacc_info.iloc[:, :2], axis=1).drop(\n",
    "            df_vacc_info.iloc[:, 4:],\n",
    "            axis=1).rename(columns={'total_vaccinations': 'Vacc_per_pop_' + each})\n",
    "        #Obtain the population for each country from the json dictionary     \n",
    "        pop = json_object_cases[each]['population']\n",
    "        #Divide by the population to obtain vaccination per population \n",
    "        df_vacc_data.iloc[:, 1] = df_vacc_data.iloc[:, 1].div(pop, axis=0)\n",
    "        #Convert date in the date dataframe to datetime format\n",
    "        df_vacc_data['date'] = pd.to_datetime(df_vacc_data['date'],\n",
    "                                              format='%Y-%m-%d')\n",
    "        # join date dateframe wit the cases per pop dataframe \n",
    "        df_vacc_list = df_vacc_list.join(\n",
    "            df_vacc_data.set_index('date'), on='date')\n",
    "        # Reset the indices for the entire dataframe     \n",
    "        df_vacc_list = df_vacc_list.reset_index(drop=True)\n",
    "     # Save the generated .csv file in the processed data folder\n",
    "    df_vacc_list.to_csv('../data/processed/Vax_per_pop.csv',\n",
    "                        sep=';', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter and Doubling Rate Calculation (Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "             date state  COUNTRY   confirmed CODE  confirmed_filtered  \\\n",
      "133877 2022-07-22    no  Germany  30331131.0  DEU          30272764.4   \n",
      "133878 2022-07-23    no  Germany  30331133.0  DEU          30341824.8   \n",
      "133879 2022-07-24    no  Germany  30331133.0  DEU          30413677.4   \n",
      "133880 2022-07-25    no  Germany  30476605.0  DEU          30481675.4   \n",
      "133881 2022-07-26    no  Germany  30598385.0  DEU          30549673.4   \n",
      "\n",
      "        confirmed_DR  confirmed_filtered_DR  \n",
      "133877  3.025988e+02             371.001523  \n",
      "133878  6.586269e+02             444.049693  \n",
      "133879  3.033113e+07             430.659422  \n",
      "133880  4.176697e+02             434.926880  \n",
      "133881  2.280148e+02             448.273117  \n"
     ]
    }
   ],
   "source": [
    "# %load ./src/get_features.py\n",
    "from scipy import signal\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input, filter_on='confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain = set(['state', 'COUNTRY', filter_on])\n",
    "    assert must_contain.issubset(set(\n",
    "        df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    pd_DR_result = df_input.groupby(['state', 'COUNTRY']).apply(\n",
    "        rolling_reg, filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result = pd_DR_result.rename(columns={filter_on: filter_on+'_DR',\n",
    "                                                'level_2': 'index'})\n",
    "\n",
    "    # we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output = pd.merge(df_input, pd_DR_result[['index', str(\n",
    "        filter_on+'_DR')]], left_index=True, right_on=['index'], how='left')\n",
    "    df_output = df_output.drop(columns=['index'])\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate'''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1, 2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array) == 3\n",
    "    reg.fit(X, y)\n",
    "    intercept = reg.intercept_\n",
    "    slope = reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input, column='confirmed', window=5):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    degree = 1\n",
    "    df_result = df_input\n",
    "\n",
    "    # attention with the neutral element here\n",
    "    filter_in = df_input[column].fillna(0)\n",
    "\n",
    "    result = signal.savgol_filter(np.array(filter_in),\n",
    "                                  window,  # window size used for filtering\n",
    "                                  1)\n",
    "    df_result[str(column+'_filtered')] = result\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def rolling_reg(df_input, col='confirmed'):\n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back = 3\n",
    "    result = df_input[col].rolling(\n",
    "        window=days_back,\n",
    "        min_periods=days_back).apply(get_doubling_time_via_regression, raw=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input, filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain = set(['state', 'COUNTRY', filter_on])\n",
    "    assert must_contain.issubset(set(\n",
    "        df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    # we need a copy here otherwise the filter_on column will be overwritten\n",
    "    df_output = df_input.copy()\n",
    "\n",
    "    pd_filtered_result = df_output[['state', 'COUNTRY', filter_on]].groupby(\n",
    "        ['state', 'COUNTRY']).apply(savgol_filter)  # .reset_index()\n",
    "\n",
    "    #print('--+++ after group by apply')\n",
    "    # print(pd_filtered_result[pd_filtered_result['country']=='Germany'].tail())\n",
    "\n",
    "    # df_output=pd.merge(df_output,pd_filtered_result[['index',str(filter_on+'_filtered')]],on=['index'],how='left')\n",
    "    df_output = pd.merge(df_output, pd_filtered_result[[str(\n",
    "        filter_on+'_filtered')]], left_index=True, right_index=True, how='left')\n",
    "    # print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg = np.array([2, 4, 6])\n",
    "    result = get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data = pd.read_csv(\n",
    "        '../data/processed/20200823_COVID_relational_confirmed.csv', sep=';', parse_dates=[0])\n",
    "    pd_JH_data = pd_JH_data.sort_values('date', ascending=True).copy()\n",
    "\n",
    "    pd_result_larg = calc_filtered_data(pd_JH_data)\n",
    "    pd_result_larg = calc_doubling_rate(pd_result_larg)\n",
    "    pd_result_larg = calc_doubling_rate(pd_result_larg, 'confirmed_filtered')\n",
    "\n",
    "    mask = pd_result_larg['confirmed'] > 100\n",
    "    pd_result_larg['confirmed_filtered_DR'] = pd_result_larg['confirmed_filtered_DR'].where(\n",
    "        mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv(\n",
    "        '../data/processed/COVID_final_set.csv', sep=';', index=False)\n",
    "    print(pd_result_larg[pd_result_larg['COUNTRY'] == 'Germany'].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIR Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Afghanistan</th>\n",
       "      <th>Albania</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Andorra</th>\n",
       "      <th>Angola</th>\n",
       "      <th>Antarctica</th>\n",
       "      <th>Antigua and Barbuda</th>\n",
       "      <th>Argentina</th>\n",
       "      <th>Armenia</th>\n",
       "      <th>...</th>\n",
       "      <th>Uruguay_fitted</th>\n",
       "      <th>Uzbekistan_fitted</th>\n",
       "      <th>Vanuatu_fitted</th>\n",
       "      <th>Venezuela_fitted</th>\n",
       "      <th>Vietnam_fitted</th>\n",
       "      <th>West Bank and Gaza_fitted</th>\n",
       "      <th>Winter Olympics 2022_fitted</th>\n",
       "      <th>Yemen_fitted</th>\n",
       "      <th>Zambia_fitted</th>\n",
       "      <th>Zimbabwe_fitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4/11/20</td>\n",
       "      <td>521</td>\n",
       "      <td>433</td>\n",
       "      <td>1825</td>\n",
       "      <td>601</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1975</td>\n",
       "      <td>967</td>\n",
       "      <td>...</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4/12/20</td>\n",
       "      <td>555</td>\n",
       "      <td>446</td>\n",
       "      <td>1914</td>\n",
       "      <td>638</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2142</td>\n",
       "      <td>1013</td>\n",
       "      <td>...</td>\n",
       "      <td>505.216780</td>\n",
       "      <td>776.124335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.971012</td>\n",
       "      <td>259.097991</td>\n",
       "      <td>271.951948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.015977</td>\n",
       "      <td>40.426600</td>\n",
       "      <td>14.134546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4/13/20</td>\n",
       "      <td>607</td>\n",
       "      <td>467</td>\n",
       "      <td>1983</td>\n",
       "      <td>646</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2208</td>\n",
       "      <td>1039</td>\n",
       "      <td>...</td>\n",
       "      <td>509.426393</td>\n",
       "      <td>785.356621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.079643</td>\n",
       "      <td>260.189376</td>\n",
       "      <td>275.961534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.032209</td>\n",
       "      <td>40.857471</td>\n",
       "      <td>14.270351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4/14/20</td>\n",
       "      <td>665</td>\n",
       "      <td>475</td>\n",
       "      <td>2070</td>\n",
       "      <td>659</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2277</td>\n",
       "      <td>1067</td>\n",
       "      <td>...</td>\n",
       "      <td>513.627717</td>\n",
       "      <td>794.698123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.329494</td>\n",
       "      <td>261.273985</td>\n",
       "      <td>280.029581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.048700</td>\n",
       "      <td>41.292651</td>\n",
       "      <td>14.407426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4/15/20</td>\n",
       "      <td>770</td>\n",
       "      <td>494</td>\n",
       "      <td>2160</td>\n",
       "      <td>673</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2443</td>\n",
       "      <td>1111</td>\n",
       "      <td>...</td>\n",
       "      <td>517.819615</td>\n",
       "      <td>804.150117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.724248</td>\n",
       "      <td>262.351650</td>\n",
       "      <td>284.156920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.065455</td>\n",
       "      <td>41.732177</td>\n",
       "      <td>14.545783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 399 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Afghanistan  Albania  Algeria  Andorra  Angola  Antarctica  \\\n",
       "80  4/11/20          521      433     1825      601      19           0   \n",
       "81  4/12/20          555      446     1914      638      19           0   \n",
       "82  4/13/20          607      467     1983      646      19           0   \n",
       "83  4/14/20          665      475     2070      659      19           0   \n",
       "84  4/15/20          770      494     2160      673      19           0   \n",
       "\n",
       "    Antigua and Barbuda  Argentina  Armenia  ...  Uruguay_fitted  \\\n",
       "80                   21       1975      967  ...      501.000000   \n",
       "81                   21       2142     1013  ...      505.216780   \n",
       "82                   23       2208     1039  ...      509.426393   \n",
       "83                   23       2277     1067  ...      513.627717   \n",
       "84                   23       2443     1111  ...      517.819615   \n",
       "\n",
       "    Uzbekistan_fitted  Vanuatu_fitted  Venezuela_fitted  Vietnam_fitted  \\\n",
       "80         767.000000             0.0        175.000000      258.000000   \n",
       "81         776.124335             0.0        179.971012      259.097991   \n",
       "82         785.356621             0.0        185.079643      260.189376   \n",
       "83         794.698123             0.0        190.329494      261.273985   \n",
       "84         804.150117             0.0        195.724248      262.351650   \n",
       "\n",
       "    West Bank and Gaza_fitted  Winter Olympics 2022_fitted  Yemen_fitted  \\\n",
       "80                 268.000000                          0.0      1.000000   \n",
       "81                 271.951948                          0.0      1.015977   \n",
       "82                 275.961534                          0.0      1.032209   \n",
       "83                 280.029581                          0.0      1.048700   \n",
       "84                 284.156920                          0.0      1.065455   \n",
       "\n",
       "    Zambia_fitted  Zimbabwe_fitted  \n",
       "80      40.000000        14.000000  \n",
       "81      40.426600        14.134546  \n",
       "82      40.857471        14.270351  \n",
       "83      41.292651        14.407426  \n",
       "84      41.732177        14.545783  \n",
       "\n",
       "[5 rows x 399 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load ./src/sir_modeling.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "\n",
    "#Suppress warnings if any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Read the csv file for sir model\n",
    "df_input_large = pd.read_csv(\n",
    "    '../data/processed/COVID_sir_flat_table.csv', sep=';').iloc[80:]\n",
    "pop = pd.read_csv('../data/processed/population.csv', sep=';')\n",
    "\n",
    "df_all = df_input_large.columns\n",
    "df_all = list(df_all)\n",
    "\n",
    "#ode definition for the SIR model\n",
    "def SIR_model(SIR, beta, gamma):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "\n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "\n",
    "    '''\n",
    "\n",
    "    S, I, R = SIR\n",
    "    dS_dt = -beta*S*I/N0  # S*I is the\n",
    "    dI_dt = beta*S*I/N0-gamma*I\n",
    "    dR_dt = gamma*I\n",
    "    return([dS_dt, dI_dt, dR_dt])\n",
    "\n",
    "\n",
    "# Functions for SIR model with time step\n",
    "def SIR_model_t(SIR, t, beta, gamma):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        t: time step, mandatory for integral.odeint\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "\n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "\n",
    "    '''\n",
    "\n",
    "    S, I, R = SIR\n",
    "    dS_dt = -beta*S*I/N0  # S*I is the\n",
    "    dI_dt = beta*S*I/N0-gamma*I\n",
    "    dR_dt = gamma*I\n",
    "    return dS_dt, dI_dt, dR_dt\n",
    "\n",
    "\n",
    "# Function defined for optimize curve fit\n",
    "def fit_odeint(x, beta, gamma):\n",
    "    '''\n",
    "    helper function for the integration\n",
    "    '''\n",
    "    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:, 1]  # we only would like to get dI\n",
    "\n",
    "\n",
    "# Fitting parameter for SIR model\n",
    "for each in df_all[1:]:\n",
    "    ydata = np.array(df_input_large[each])\n",
    "    t = np.arange(len(ydata))\n",
    "    N0 = 6000000  # max susceptible population\n",
    "\n",
    "    # ensure re-initialization\n",
    "    I0 = ydata[0]\n",
    "    S0 = N0-I0\n",
    "    R0 = 0\n",
    "\n",
    "    popt, pcov = optimize.curve_fit(fit_odeint, t, ydata, maxfev=1600000)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    # get the final fitted curve\n",
    "    fitted = fit_odeint(t, *popt).reshape(-1, 1)\n",
    "    df_input_large[each + '_fitted'] = fitted\n",
    "\n",
    "#Saving the fitted SIR model as .csv in the respective path\n",
    "df_input_large.to_csv('../data/processed/COVID_sir_fitted_table.csv', sep=';')\n",
    "df_input_large.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Exists: Fetch the latest data from repository\n",
      "Dash is running on http://127.0.0.1:8051/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8051/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8051/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# %load ./src/dashboard.py\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash import html\n",
    "from dash import dcc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Suppress the warnings for depreceated packages \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get current working directory path and append it\n",
    "path = (os.getcwd()+'\\\\src\\\\')\n",
    "sys.path.append(path)\n",
    "import import_data\n",
    "\n",
    "#Get the country list and iso codes \n",
    "list_cases_country, df_country_info = import_data.import_cases_data()\n",
    "\n",
    "df_input_large = pd.read_csv('../data/processed/COVID_final_set.csv', sep=';')\n",
    "df = pd.read_csv('../data/processed/COVID_CRD.csv', sep=';')\n",
    "\n",
    "# Read the csv's for the generates sir model for all countries \n",
    "df_input_sir = pd.read_csv(\n",
    "    '../data/processed/COVID_sir_fitted_table.csv', sep=';')\n",
    "df_all = df_input_sir.columns\n",
    "df_all = list(df_all[:109])\n",
    "\n",
    "#Read the csv's for the list of cases and list of vaccination\n",
    "df_list = pd.read_csv('../data/processed/Cases_pop_NoNaN.csv', sep=';')\n",
    "df_vacc_list = pd.read_csv('../data/processed/Vax_per_pop.csv', sep=';')\n",
    "\n",
    "#Parse the country name and iso codes \n",
    "country_name = df_country_info['location'].unique()\n",
    "country_iso_code = df_country_info['iso_code'].unique()\n",
    "\n",
    "\n",
    "'''Dashboard is created by using an external stylesheet named BOOTSTRAP. \n",
    "BOOTSTRAP allows us to divide the dashboard into Rows and columns.\n",
    "COVID-19 dashbord has 5 Rows and 2 columns'''\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app.title = 'COVID-19 Dashboard'\n",
    "\n",
    "app.layout = html.Div([\n",
    "    # First Row: Information regarding dashboard page\n",
    "    dbc.Row(dbc.Col(html.Div(dcc.Markdown('''\n",
    "                            # Applied Data Science: COVID-19 Data Analysis\n",
    "                            Project Goals:\n",
    "                            * Generate the plots for COVID cases for all countries \n",
    "                            * Doubling rate calculation.\n",
    "                            * Simulation of COVID spread for countries using SIR model.\n",
    "                            * Dashboard creation for cases, relative cases per population. Vaccination per population and SIR model\n",
    "                            ''',style={\n",
    "                            'fontFamily': 'sans-serif',\n",
    "                            'textAlign': 'left',\n",
    "                            'backgroundColor':'#377eb8',\n",
    "                            'margin': '5px',\n",
    "                            'color': '#F9690E',\n",
    "                            'padding': '5px',\n",
    "                            'borderRadius': '5px'})),\n",
    "                    width={'size': 15, 'offset': 0},\n",
    "                    )\n",
    "            ),\n",
    "    # Second Row: Dropdowns for first two graphs\n",
    "    dbc.Row(\n",
    "        [  # Dropdown for Timeline Confirmed and Doubling rate\n",
    "            dbc.Col(dcc.Dropdown(\n",
    "                    id='country_dropdown',\n",
    "                    options=[{'label': each, 'value': each}\n",
    "                             for each in df_input_large['COUNTRY'].unique()],\n",
    "                    # which are pre-selected\n",
    "                    value=['Japan', 'Germany', 'India'],\n",
    "                    multi=True),\n",
    "\n",
    "                    width={'size': 5, \"offset\": 0, 'order': 'first'}\n",
    "                    ),\n",
    "            #Dropdown for the list of cases per population\n",
    "            dbc.Col(dcc.Dropdown(\n",
    "                    id='country_drop_down',\n",
    "                    options=[{'label': country_name[each], 'value':country_iso_code[each]}\n",
    "                             for each in range(len(country_name))],\n",
    "                    value=['USA', 'IND'],\n",
    "                    multi=True),\n",
    "\n",
    "                    width={'size': 5, \"offset\": 6, 'order': 'first'}\n",
    "                    ),\n",
    "            #Dropdown for the timeline,doubling time and filtered data\n",
    "            dbc.Col(\n",
    "                dcc.Dropdown(\n",
    "                    id='doubling_time',\n",
    "                    options=[\n",
    "                        {'label': 'Timeline Confirmed ',\n",
    "                         'value': 'confirmed'},\n",
    "                        {'label': 'Timeline Confirmed Filtered',\n",
    "                         'value': 'confirmed_filtered'},\n",
    "                        {'label': 'Timeline Doubling Rate',\n",
    "                         'value': 'confirmed_DR'},\n",
    "                        {'label': 'Timeline Doubling Rate Filtered',\n",
    "                         'value': 'confirmed_filtered_DR'}\n",
    "                    ],\n",
    "                    value='confirmed',\n",
    "                    multi=False\n",
    "                ),\n",
    "                width={'size': 3, \"offset\": 0, 'order': 'second'}\n",
    "            ),\n",
    "\n",
    "        ], className=\"g-0\",\n",
    "        # style=dict(display='flex')\n",
    "    ),\n",
    "\n",
    "    # Third Row: Graphs for cases/confirmed cases/Doubling rate and graph for cases per population:\n",
    "    dbc.Row(\n",
    "        [   #Graph for cases and cases per population \n",
    "            dbc.Col(dcc.Graph(\n",
    "                    id='main_window_slope'\n",
    "                    ),\n",
    "                    width=6, md={'size': 5,  \"offset\": 0, 'order': 'first'}\n",
    "                    ),\n",
    "\n",
    "            dbc.Col(dcc.Graph(\n",
    "                    id='cases_per_pop'\n",
    "                    ),\n",
    "                    width=6, md={'size': 5,  \"offset\": 1, 'order': 'first'}\n",
    "                    ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    dbc.Row(\n",
    "        [   #Dropdown for vaccination per population \n",
    "            dbc.Col(dcc.Dropdown(\n",
    "                id='country_vacc_data',\n",
    "                options=[{'label': country_name[each], 'value':country_iso_code[each]}\n",
    "                         for each in range(len(country_name))],\n",
    "                value=['USA', 'IND'],\n",
    "                multi=True),\n",
    "\n",
    "                width={'size': 5, \"offset\": 0, 'order': 'second'}\n",
    "            ),\n",
    "\n",
    "            # Dropdown for SIR model\n",
    "            dbc.Col(dcc.Dropdown(\n",
    "                    id='country_dropdown_sir',\n",
    "                    options=[{'label': each, 'value': each}\n",
    "                             for each in df_all[1:]],\n",
    "                    value='Brazil',  # which are pre-selected\n",
    "                    multi=False\n",
    "                    ),\n",
    "\n",
    "                    width={'size': 5, \"offset\": 6, 'order': 'second'}\n",
    "                    ),\n",
    "        ], className=\"g-0\",\n",
    "    ),\n",
    "    dbc.Row(\n",
    "        [   # Graph plot for vaccination per population \n",
    "            dbc.Col(dcc.Graph(\n",
    "                    id='vacc_data'\n",
    "                    ),\n",
    "                    width=6, md={'size': 5,  \"offset\": 0, 'order': 'first'}\n",
    "                    ),\n",
    "\n",
    "            dbc.Col(dcc.Graph(\n",
    "                    id='SIR_model'\n",
    "                    ),\n",
    "                    width=6, md={'size': 5,  \"offset\": 1, 'order': 'first'}\n",
    "                    ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    dbc.Row(\n",
    "        #Generation of the world map graph \n",
    "        dbc.Col(dcc.Graph(id=\"World_map\",\n",
    "                          figure=go.Figure(data=[go.Choropleth(\n",
    "                              locations=df['CODE'],\n",
    "                              z=df['Confirm cases'],\n",
    "                              text=df['COUNTRY'],\n",
    "                              colorscale='Blues',\n",
    "                              autocolorscale=False,\n",
    "                              reversescale=False,\n",
    "                              marker_line_color='darkgray',\n",
    "                              marker_line_width=0.5,\n",
    "                              colorbar_title='Confirmed cases'\n",
    "                          )],\n",
    "                              layout=go.Layout(\n",
    "                              title_text='COVID 19 WORLD MAP',\n",
    "                              height=1300,\n",
    "                              autosize=True,\n",
    "                              geo=dict(\n",
    "                                  showframe=False,\n",
    "                                  showcoastlines=False,\n",
    "                                  projection_type='equirectangular'\n",
    "                              ))\n",
    "                          ),\n",
    "\n",
    "                          ),\n",
    "                width=12, md={'size': 12,  \"offset\": 0, 'order': 'first'}\n",
    "                ),\n",
    "    )\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "# Figure definition for cases per population \n",
    "@app.callback(Output('cases_per_pop', 'figure'),\n",
    "              [Input('country_drop_down', 'value')])\n",
    "def Cases_fig(list_cases_country):\n",
    "\n",
    "    traces = []\n",
    "    # Browsing over the list of countries and appending the plots\n",
    "    for each in list_cases_country:\n",
    "        traces.append(\n",
    "            dict(x=df_list.date,\n",
    "                 y=df_list['Cases_per_pop_' + each],\n",
    "                 mode='markers+lines',\n",
    "                 opacity=0.9,\n",
    "                 line_width=2,\n",
    "                 marker_size=1,\n",
    "                 name=each))\n",
    "\n",
    "    #Figure dimensions for the plots \n",
    "    return {\n",
    "        'data':\n",
    "        traces,\n",
    "        'layout':\n",
    "        dict(width=1280,\n",
    "             height=900,\n",
    "             title='Plot for Cases per Population',\n",
    "             xaxis={'title': 'Date',\n",
    "                    'tickangle': -45,\n",
    "                    'nticks': 20,\n",
    "                    'tickfont': dict(size=14, color='#7f7f7f'),\n",
    "                    },\n",
    "             yaxis={'title': 'Relative COVID Cases (Absolute Cases/Total Population)',\n",
    "                    'type': 'log',\n",
    "                    'range': '[1.1, 5.5]'\n",
    "                    })\n",
    "    }\n",
    "\n",
    "\n",
    "#Figure definition for generating vaccination per population plots \n",
    "@app.callback(Output('vacc_data', 'figure'),\n",
    "              [Input('country_vacc_data', 'value')])\n",
    "def Vacc_fig(list_cases_country):\n",
    "\n",
    "    traces = []\n",
    "    #Browsing over the list of countries and appending the plots \n",
    "    for each in list_cases_country:\n",
    "        traces.append(\n",
    "            dict(x=df_vacc_list.date,\n",
    "                 y=df_vacc_list['Vacc_per_pop_' + each],\n",
    "                 mode='markers+lines',\n",
    "                 opacity=0.9,\n",
    "                 line_width=2,\n",
    "                 marker_size=1,\n",
    "                 name=each))\n",
    "\n",
    "    #Figure dimensions for the plots \n",
    "    return {\n",
    "        'data':\n",
    "        traces,\n",
    "        'layout':\n",
    "        dict(width=1280,\n",
    "             height=900,\n",
    "             title='Plot for Vaccination Data',\n",
    "             xaxis={'title': 'Date',\n",
    "                    'tickangle': -45,\n",
    "                    'nticks': 20,\n",
    "                    'tickfont': dict(size=14, color='#7f7f7f'),\n",
    "                    },\n",
    "             yaxis={'title': 'Relative Vaccination(Total Vaccination/Total Population)',\n",
    "                    'type': 'log',\n",
    "                    'range': '[1.1, 5.5]'\n",
    "                    })\n",
    "    }\n",
    "\n",
    "#Figure definition for generating confirmed, filtered and doubling rate plots \n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_dropdown', 'value'),\n",
    "     Input('doubling_time', 'value')])\n",
    "def update_figure(country_list, show_doubling):\n",
    "    # Condition to change the y axis title based on doubling rate being present or not \n",
    "    if 'DR' in show_doubling:\n",
    "        my_yaxis = {'type': \"log\",\n",
    "                    'title': 'Approximated doubling rate over 3 days (larger numbers are better #stayathome)'\n",
    "                    }\n",
    "    else:\n",
    "        my_yaxis = {'type': \"log\",\n",
    "                    'title': 'Confirmed infected people (source johns hopkins csse, log-scale)'\n",
    "                    }\n",
    "\n",
    "    traces = []\n",
    "    #Browsing over the list of countries and appending the plots     \n",
    "    for each in country_list:\n",
    "\n",
    "        df_plot = df_input_large[df_input_large['COUNTRY'] == each]\n",
    "\n",
    "        if show_doubling == 'doubling_rate_filtered':\n",
    "            df_plot = df_plot[['state', 'COUNTRY', 'confirmed', 'confirmed_filtered', 'confirmed_DR',\n",
    "                               'confirmed_filtered_DR', 'date']].groupby(['COUNTRY', 'date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot = df_plot[['state', 'COUNTRY', 'confirmed', 'confirmed_filtered', 'confirmed_DR',\n",
    "                               'confirmed_filtered_DR', 'date']].groupby(['COUNTRY', 'date']).agg(np.sum).reset_index()\n",
    "       # print(show_doubling)\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                           y=df_plot[show_doubling],\n",
    "                           mode='markers+lines',\n",
    "                           opacity=0.9,\n",
    "                           name=each\n",
    "                           )\n",
    "                      )\n",
    "    #Figure dimensions for the plots \n",
    "    return {\n",
    "        'data': traces,\n",
    "        'layout': dict(\n",
    "            width=1280,\n",
    "            height=900,\n",
    "            xaxis={'title': 'Timeline',\n",
    "                   'tickangle': -45,\n",
    "                   'nticks': 20,\n",
    "                   'tickfont': dict(size=14, color=\"#7f7f7f\"),\n",
    "                   },\n",
    "\n",
    "            yaxis=my_yaxis\n",
    "        )\n",
    "    }\n",
    "\n",
    "#Figure definition for generating SIR plots \n",
    "@app.callback(\n",
    "    Output('SIR_model', 'figure'),\n",
    "    [Input('country_dropdown_sir', 'value')])\n",
    "def SIR_fig(con_input):\n",
    "    df = df_input_sir\n",
    "\n",
    "    #Browsing over the dataframe and appending the plots\n",
    "    for i in df[1:]:\n",
    "        data = []\n",
    "        trace = go.Scatter(x=df.date,\n",
    "                           y=df[con_input],\n",
    "                           mode='lines+markers',\n",
    "                           name=con_input)\n",
    "        data.append(trace)\n",
    "\n",
    "        trace_fitted = go.Scatter(x=df.date,\n",
    "                                  y=df[con_input + '_fitted'],\n",
    "                                  mode='lines+markers',\n",
    "                                  name=con_input+'_fitted')\n",
    "        data.append(trace_fitted)\n",
    "        \n",
    "    #Figure dimensions for the plots \n",
    "    return {'data': data,\n",
    "            'layout': dict(\n",
    "                width=1280,\n",
    "                height=900,\n",
    "                title='SIR model',\n",
    "                xaxis={'tickangle': -45,\n",
    "                       'nticks': 20,\n",
    "                       'tickfont': dict(size=14, color=\"#7f7f7f\"),\n",
    "                       },\n",
    "                yaxis={'type': \"log\",\n",
    "                       'range': '[1.1,5.5]'\n",
    "                       }\n",
    "\n",
    "            )\n",
    "            }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, port=8051, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
